{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "Creating a data lake for immigration activity data in order to be able to make analytical queries on the star schema to be created. \n",
    "Also we will be able to make ad hoc queries in the raw data (schema-on read) since raw data reside on the same space. \n",
    "\n",
    "**! ! ! The assessor can run this whole project because the data lake is created locally in parquet files ! ! !**\n",
    "\n",
    "**All the assessor needs to do is to go through this Jupiter notebook.**\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, TimestampType, StringType, StructField\n",
    "from pyspark.sql.functions import udf, col, date_add\n",
    "\n",
    "from helpful_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Instatiate Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set pandas dataframes dispay columns to unlimited\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "We are creating a data warehouse as part of a data lake where the analytics team would be able to analyze immigration movement and specifically which points of entry(e.g. which airports) are mostly involved. With that in mind the data model is designed in a way that supports flexibility on analytical queries. Queries like \"Top 10 points of entry that immigrants enter a country\".\n",
    "\n",
    "We make it easy and efficient for the team to write these queries as there is at most one JOIN operation included (advantage of the denormalized form) to get a piece of information (e.g. join country code with countries dimension to find a country's name).\n",
    "\n",
    "Our data warehouse will be written into parquet files in order to take advantage of their columnar storage for better performance on fetching column data. Also we gain storage efficiency as compressed columnar files take less space.\n",
    "\n",
    "In addition we keep the raw data in the same space. This serves well the data scientists and machine learning engineers since they have a lot of flexibility on reading the data however they want and they can run ad-hoc queries on the raw data directly (schema-on-read). Therefore our data lake can serve Data Scientists and ML Engineers along with business analysts in comparison with a traditional data warehouse which could only serve business analysts.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "##### Datasets:\n",
    "**Immigration data**: Provided by Udacity. This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/national-travel-and-tourism-office). They contain immigration information for individuals e.g. the mode of entry (sea, air, land).\n",
    "\n",
    "**Airport data**: Provided by Udacity. This data contains airport codes and corresponding cities and they come from [datahub.io](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "**I94 Labels**: Provided by Udacity. These data come from I94_SAS_Labels_Descriptions.SAS file and they contain codes and their descriptions (e.g. city codes and the actual city names). In the following block of code we parse the content of this file in order to create csv datasets, for the codes we choose in order to read them as dataframes.\n",
    "\n",
    "*****The Immigration Data will be used to create our facts table while the other two datasets will be used for the dimensions.*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create csv files out of the codes and save them in label_data_sources folder\n",
    "# I choose to create csv files for only state, country and mode of entry codes\n",
    "#(labelled with 'i94addrl', 'i94cntyl' and 'i94model' respectively ).\n",
    "create_data_sources_from_label_file('I94_SAS_Labels_Descriptions.SAS', 'label_data_sources', 'i94addrl', 'i94cntyl', 'i94model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa  \\\n",
       "0      CA  20582.0    40.0      1.0    1.0  20160430      SYD  None       G   \n",
       "1      NV  20591.0    32.0      1.0    1.0  20160430      SYD  None       G   \n",
       "2      WA  20582.0    29.0      1.0    1.0  20160430      SYD  None       G   \n",
       "3      WA  20588.0    29.0      1.0    1.0  20160430      SYD  None       G   \n",
       "4      WA  20588.0    28.0      1.0    1.0  20160430      SYD  None       G   \n",
       "\n",
       "  entdepd entdepu matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0       O    None       M   1976.0  10292016      F   None      QF   \n",
       "1       O    None       M   1984.0  10292016      F   None      VA   \n",
       "2       O    None       M   1987.0  10292016      M   None      DL   \n",
       "3       O    None       M   1987.0  10292016      F   None      DL   \n",
       "4       O    None       M   1988.0  10292016      M   None      DL   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  9.495387e+10  00011       B1  \n",
       "1  9.495562e+10  00007       B1  \n",
       "2  9.495641e+10  00040       B1  \n",
       "3  9.495645e+10  00040       B1  \n",
       "4  9.495639e+10  00040       B1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read immigration data\n",
    "immigration_data = './sas_data'\n",
    "immigration_df = spark.read.load(immigration_data)\n",
    "\n",
    "immigration_df.createOrReplaceTempView(\"immigration_staging\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM immigration_staging\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport           11   \n",
       "1  00AA  small_airport                Aero B Ranch Airport         3435   \n",
       "2  00AK  small_airport                        Lowell Field          450   \n",
       "3  00AL  small_airport                        Epps Airpark          820   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport          237   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NA          US      US-PA      Bensalem      00A      None   \n",
       "1        NA          US      US-KS         Leoti     00AA      None   \n",
       "2        NA          US      US-AK  Anchor Point     00AK      None   \n",
       "3        NA          US      US-AL       Harvest     00AL      None   \n",
       "4        NA          US      US-AR       Newport     None      None   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4       None                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the airport data here\n",
    "airport_data = 'airport-codes_csv.csv'\n",
    "airport_df = spark.read.csv(airport_data, header=True)\n",
    "\n",
    "airport_df.createOrReplaceTempView(\"airports_staging\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM airports_staging\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code       state\n",
       "0         AL     ALABAMA\n",
       "1         AK      ALASKA\n",
       "2         AZ     ARIZONA\n",
       "3         AR    ARKANSAS\n",
       "4         CA  CALIFORNIA"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the state data here\n",
    "state_data = 'label_data_sources/addr.csv'\n",
    "schema = StructType([\n",
    "    StructField(\"state_code\", StringType()),\n",
    "    StructField(\"state\", StringType())\n",
    "])\n",
    "\n",
    "state_df = spark.read.csv(state_data, header=False, schema=schema)\n",
    "state_df.createOrReplaceTempView(\"state_codes\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM state_codes\n",
    "\"\"\"\n",
    ").toPandas().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode_code</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mode_code          mode\n",
       "0          1           Air\n",
       "1          2           Sea\n",
       "2          3          Land\n",
       "3          9  Not reported"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the entry mode data here\n",
    "\n",
    "mode_data = 'label_data_sources/mode.csv'\n",
    "schema = StructType([\n",
    "    StructField(\"mode_code\", IntegerType()),\n",
    "    StructField(\"mode\", StringType())\n",
    "])\n",
    "\n",
    "mode_df = spark.read.csv(mode_data, header=False, schema=schema)\n",
    "mode_df.createOrReplaceTempView(\"mode_codes\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM mode_codes\n",
    "\"\"\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_code                                            country\n",
       "0           582  MEXICO Air Sea, and Not Reported (I-94, no lan...\n",
       "1           236                                        AFGHANISTAN\n",
       "2           101                                            ALBANIA\n",
       "3           316                                            ALGERIA\n",
       "4           102                                            ANDORRA"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the country data here\n",
    "\n",
    "country_data = 'label_data_sources/cnty.csv'\n",
    "schema = StructType([\n",
    "    StructField(\"country_code\", IntegerType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "addr_df = spark.read.csv(country_data, header=False, schema=schema)\n",
    "addr_df.createOrReplaceTempView(\"country_codes\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM country_codes\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "1. From a quick look on the immigration_df above we can see that cicid, i94yr, i94mon, i94res, i94mode, biryear, count are float so we will need to cast them as integers (some of these will also be used to join with the dimension tables on integer values)\n",
    "\n",
    "2. We also need to check if cicid in immigration data is unique in order to ensure that it uniquely identifies an immigration movement record.\n",
    "\n",
    "3. Check that the ***iata_code*** field in the airport codes dataset actually matches with the ***i94port*** field of the immiggration data so that we know if airport_codes.csv serves our use case. I have actually performed this check before I chose my datasets in order to ensure that this dataset can be joined with the immigration dataset so that it suits my purpose.\n",
    "\n",
    "4. Check for null values on the fields i94res, i94mode , i94port, i94addr which will be the dimension fields in our facts table. \n",
    "\n",
    "5. Also check for null values and duplicated values in iata_code field of airport codes dataset.\n",
    "\n",
    "6. Check for duplicates in country_df, state_df. \n",
    "\n",
    "7. Check counts before and after cleaning the data.\n",
    "\n",
    "Below we perform the above checks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 1 \n",
    "From a quick look on the immigration_df above we can see that cicid, i94yr, i94mon, i94res, i94mode, biryear, count are float so we will need to cast them as integers (some of these will also be used to join with the dimension tables on integer values).\n",
    "\n",
    "This check has been done above by looking the at the displayed immigration data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 2\n",
    "We also need to check if cicid in immigration data is unique in order to ensure that uniquely identifies an immigration activity record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n",
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count all the records\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM immigration_staging\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 3096313\n",
    "\n",
    "# count unique cicid values\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT cicid)\n",
    "FROM immigration_staging\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "# The counts are the same (3096313) therefore cicid uniquely identifies an immigration record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 3\n",
    "Check that the ***iata_code*** field in the airport codes dataset actually matches with the ***i94port*** field of the immigration data so that we know if airport_codes.csv serves our use case. I have actually performed this check before I choose my datasets in order to ensure that this dataset can be joined with the immigration dataset so that it suits my purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all the iata_code values as a set\n",
    "iata_codes = spark.sql(\"\"\"\n",
    "SELECT DISTINCT iata_code\n",
    "FROM airports_staging\n",
    "\"\"\"\n",
    ")\n",
    "iata_codes_set = set(iata_codes.rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Getting all the i94port values as a set\n",
    "i94port_codes = spark.sql(\"\"\"\n",
    "SELECT DISTINCT i94port\n",
    "FROM immigration_staging\n",
    "\"\"\"\n",
    ")\n",
    "i94port_codes_set = set(i94port_codes.rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# checking if there is an intersection between the two sets\n",
    "len(iata_codes_set.intersection(i94port_codes_set))\n",
    "# Returns 251 therefore there is intersection thus we can join i94port with iata_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 4. \n",
    "Check for null values on the fields i94res, i94mode , i94port, i94addr which will be the dimension join fields in our facts table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+\n",
      "|i94res|i94mode|i94port|i94addr|\n",
      "+------+-------+-------+-------+\n",
      "|     0|    239|      0| 152592|\n",
      "+------+-------+-------+-------+\n",
      "\n",
      "+------+------+\n",
      "|i94res| count|\n",
      "+------+------+\n",
      "| 101.0|   929|\n",
      "| 102.0|   117|\n",
      "| 103.0| 15465|\n",
      "| 104.0| 20796|\n",
      "| 105.0|  2343|\n",
      "| 107.0| 16153|\n",
      "| 108.0| 24600|\n",
      "| 109.0|  1983|\n",
      "| 110.0| 11545|\n",
      "| 111.0|185339|\n",
      "| 112.0|156613|\n",
      "| 113.0|  7251|\n",
      "| 114.0|  5835|\n",
      "| 115.0|  3922|\n",
      "| 116.0| 29894|\n",
      "| 117.0| 65782|\n",
      "| 118.0|  1475|\n",
      "| 119.0|   168|\n",
      "| 120.0|  2316|\n",
      "| 121.0|  1417|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+\n",
      "|i94mode|  count|\n",
      "+-------+-------+\n",
      "|   null|    239|\n",
      "|    1.0|2994505|\n",
      "|    2.0|  26349|\n",
      "|    3.0|  66660|\n",
      "|    9.0|   8560|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-----+\n",
      "|i94port|count|\n",
      "+-------+-----+\n",
      "|    5KE|    3|\n",
      "|    5T6|    4|\n",
      "|    ABG|   11|\n",
      "|    ABQ|    3|\n",
      "|    ABS|    4|\n",
      "|    ADS|   15|\n",
      "|    ADT|   10|\n",
      "|    ADW|    4|\n",
      "|    AGA|80919|\n",
      "|    AGN|   11|\n",
      "|    ALC|   39|\n",
      "|    ANA|    1|\n",
      "|    ANC|   91|\n",
      "|    AND|   57|\n",
      "|    ANZ|  677|\n",
      "|    APF|    9|\n",
      "|    ATL|92579|\n",
      "|    ATW|    3|\n",
      "|    AUS| 3034|\n",
      "|    AXB|   85|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------+\n",
      "|i94addr| count|\n",
      "+-------+------+\n",
      "|   null|152592|\n",
      "|     ..|    30|\n",
      "|     .C|    17|\n",
      "|     .D|     2|\n",
      "|     .I|     4|\n",
      "|     .L|     2|\n",
      "|     .M|     2|\n",
      "|     .N|     4|\n",
      "|     .T|     6|\n",
      "|      0|    26|\n",
      "|     00|     1|\n",
      "|     02|     1|\n",
      "|     03|     1|\n",
      "|     06|     1|\n",
      "|     10|    12|\n",
      "|     11|     4|\n",
      "|     14|     1|\n",
      "|     18|     1|\n",
      "|     20|     5|\n",
      "|     21|     4|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# found this piece of code in https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe\n",
    "immigration_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in ['i94res', 'i94mode' , 'i94port', 'i94addr']]).show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94res, COUNT(*) AS count\n",
    "FROM immigration_staging\n",
    "GROUP BY i94res\n",
    "ORDER BY i94res\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94mode, COUNT(*) AS count\n",
    "FROM immigration_staging\n",
    "GROUP BY i94mode\n",
    "ORDER BY i94mode\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94port, COUNT(*) AS count\n",
    "FROM immigration_staging\n",
    "GROUP BY i94port\n",
    "ORDER BY i94port\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94addr, COUNT(*) AS count\n",
    "FROM immigration_staging\n",
    "GROUP BY i94addr\n",
    "ORDER BY i94addr\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Conclusions on check 4 above: \n",
    "\n",
    "We can see that some of our chosen dimensions contain null values. Also they contain weird formatted codes that definitely do not exist in the codes datasets.\n",
    "Therefore we will use left joins with the dimension data when we create our facts table.\n",
    "Since I want to keep all the information on the immigration data I will not remove any of the records containing null or weird codes in the above fields.\n",
    "But if I wanted to \"clean\" these data I would use joins of the immigration data with all the dimensions, for the creation of my fact table only the values that are matched are kept in the fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 5\n",
    "Also check for null values and duplicated values in iata_code field of airport codes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|iata_code|count|\n",
      "+---------+-----+\n",
      "|     null|45886|\n",
      "|        0|   80|\n",
      "|      AHT|    2|\n",
      "|      AUS|    2|\n",
      "|      BCK|    2|\n",
      "|      BFC|    2|\n",
      "|      BVW|    2|\n",
      "|      CLG|    2|\n",
      "|      CMN|    2|\n",
      "|      CQP|    2|\n",
      "|      CSZ|    2|\n",
      "|      CTR|    2|\n",
      "|      DDU|    2|\n",
      "|      DLR|    2|\n",
      "|      DZI|    2|\n",
      "|      ESP|    2|\n",
      "|      GGC|    2|\n",
      "|      GVA|    2|\n",
      "|      HAT|    2|\n",
      "|      HDB|    2|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT iata_code, COUNT(*) AS count\n",
    "FROM airports_staging\n",
    "GROUP BY iata_code\n",
    "having count > 1\n",
    "ORDER BY iata_code\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Remarks on check 5 above: \n",
    "\n",
    "We can see that iata_code has null values on the iata code field.\n",
    "Also there are codes that appear more than once. \n",
    "We will remove the records with null iata_codes and deduplicate the ones that appear more than once in the cleaning steps.\n",
    "We do that because we want iata_code to be unique (dimension primary key).\n",
    "\n",
    "The below steps for check 5 will be used in the cleaning process of airport_codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We will take only the codes that appear once in the dataset and check if still the dataset is sufficient enough (i.e contains enough records to match immigration data)\n",
    "\n",
    "# getting all the iata_codes that appear only once\n",
    "non_duplicate_iata_codes = spark.sql(\"\"\"\n",
    "select iata_code\n",
    "from\n",
    "(SELECT iata_code, COUNT(*) AS count\n",
    "FROM airports_staging\n",
    "GROUP BY iata_code\n",
    "having count = 1\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "non_duplicate_iata_codes.createOrReplaceTempView(\"non_duplicate_iata_codes\")\n",
    "\n",
    "# selecting the records that their iata_codes appear only once\n",
    "dedup_airports = spark.sql(\"\"\"\n",
    "select * \n",
    "from airports_staging\n",
    "where iata_code in (select * from non_duplicate_iata_codes)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "dedup_airports.createOrReplaceTempView(\"dedup_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that the iata_code field in the deduplicated airport codes dataset (dedup_airports) actually still matches with  enough records from the immigration data\n",
    "\n",
    "iata_codes = spark.sql(\"\"\"\n",
    "SELECT DISTINCT iata_code\n",
    "FROM dedup_airports\n",
    "\"\"\"\n",
    ")\n",
    "iata_codes_set = set(iata_codes.rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Getting all the i94port values as a set\n",
    "i94port_codes = spark.sql(\"\"\"\n",
    "SELECT DISTINCT i94port\n",
    "FROM immigration_staging\n",
    "\"\"\"\n",
    ")\n",
    "i94port_codes_set = set(i94port_codes.rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# checking if there is an intersection between the two sets\n",
    "len(iata_codes_set.intersection(i94port_codes_set))\n",
    "# Returns 247 while the initial duplicated set returned 251, therefore there is a sufficient intersection thus we can join i94port with iata_codes after deduplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The above steps of check 5 will be used in the cleaning process of airport_codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 6\n",
    "Check for duplicates in country_df, state_df. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|country_code|count|\n",
      "+------------+-----+\n",
      "+------------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|state_code|count|\n",
      "+----------+-----+\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT country_code, COUNT(*) AS count\n",
    "FROM country_codes\n",
    "GROUP BY country_code\n",
    "HAVING count > 1\n",
    "ORDER BY country_code\n",
    "\"\"\"\n",
    ").show()\n",
    "# returns 0 records thus no duplicates\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT state_code, COUNT(*) AS count\n",
    "FROM state_codes\n",
    "GROUP BY state_code\n",
    "HAVING count > 1\n",
    "ORDER BY state_code\n",
    "\"\"\"\n",
    ").show()\n",
    "# returns 0 records thus no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check 7\n",
    "Check counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|immigration_count|\n",
      "+-----------------+\n",
      "|          3096313|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|airport_count|\n",
      "+-------------+\n",
      "|        55075|\n",
      "+-------------+\n",
      "\n",
      "+---------------+\n",
      "|countries_count|\n",
      "+---------------+\n",
      "|            289|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|states_count|\n",
      "+------------+\n",
      "|          55|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# immigration data\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS immigration_count\n",
    "FROM immigration_staging\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 3096313\n",
    "\n",
    "# airport data\n",
    "# This is the count before cleaning/deduplicating iata_code. \n",
    "# We have to check the count after cleaning data as well to see if there are still enough records after deduplication. \n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS airport_count\n",
    "FROM airports_staging\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 55075\n",
    "\n",
    "\n",
    "# immigration data\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS countries_count\n",
    "FROM country_codes\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 289\n",
    "\n",
    "# airport data\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS states_count\n",
    "FROM state_codes\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data.\n",
    "\n",
    "1. Cleaning Immigration data: Cast cicid, i94yr, i94mon, i94res, i94mode, biryear, count as integers. Casting arrdate into date format.\n",
    "\n",
    "2. Cleaning airport data: Deduplicate iata_code field. Check count after deduplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "***Cleaning Immigration data***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>arr_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid  i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0  5748517   2016       4   245.0     438     LOS  20574.0        1      CA   \n",
       "1  5748518   2016       4   245.0     438     LOS  20574.0        1      NV   \n",
       "2  5748519   2016       4   245.0     438     LOS  20574.0        1      WA   \n",
       "3  5748520   2016       4   245.0     438     LOS  20574.0        1      WA   \n",
       "4  5748521   2016       4   245.0     438     LOS  20574.0        1      WA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0  20582.0    40.0      1.0      1  20160430      SYD  None       G       O   \n",
       "1  20591.0    32.0      1.0      1  20160430      SYD  None       G       O   \n",
       "2  20582.0    29.0      1.0      1  20160430      SYD  None       G       O   \n",
       "3  20588.0    29.0      1.0      1  20160430      SYD  None       G       O   \n",
       "4  20588.0    28.0      1.0      1  20160430      SYD  None       G       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0    None       M     1976  10292016      F   None      QF  9.495387e+10   \n",
       "1    None       M     1984  10292016      F   None      VA  9.495562e+10   \n",
       "2    None       M     1987  10292016      M   None      DL  9.495641e+10   \n",
       "3    None       M     1987  10292016      F   None      DL  9.495645e+10   \n",
       "4    None       M     1988  10292016      M   None      DL  9.495639e+10   \n",
       "\n",
       "   fltno visatype   arr_date  \n",
       "0  00011       B1 2016-04-30  \n",
       "1  00007       B1 2016-04-30  \n",
       "2  00040       B1 2016-04-30  \n",
       "3  00040       B1 2016-04-30  \n",
       "4  00040       B1 2016-04-30  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# casting float fields to integers\n",
    "clean_immigration_df = immigration_df.withColumn(\"cicid\", col(\"cicid\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"i94yr\", col(\"i94yr\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"i94mon\", col(\"i94mon\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"i94res\", col(\"i94res\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"i94mode\", col(\"i94mode\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"biryear\", col(\"biryear\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"count\", col(\"count\").cast(IntegerType())) \n",
    "\n",
    "\n",
    "# coverting days to date format for arrival date\n",
    "get_date = udf(lambda days : datetime(1960, 1, 1) + timedelta(days=int(days)), TimestampType())\n",
    "clean_immigration_df = clean_immigration_df.withColumn(\"arr_date\", get_date(\"arrdate\")) \n",
    "\n",
    "clean_immigration_df.createOrReplaceTempView(\"clean_immigration\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM clean_immigration\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").toPandas()\n",
    "# Cleaning successfull\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "***Cleaning Airport data***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8975"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting all the iata_codes that appear only once\n",
    "non_duplicate_iata_codes = spark.sql(\"\"\"\n",
    "select iata_code\n",
    "from\n",
    "(SELECT iata_code, COUNT(*) AS count\n",
    "FROM airports_staging\n",
    "GROUP BY iata_code\n",
    "having count = 1\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "non_duplicate_iata_codes.createOrReplaceTempView(\"non_duplicate_iata_codes\")\n",
    "\n",
    "# selecting the records that their iata_codes appear only once\n",
    "clean_airports_df = spark.sql(\"\"\"\n",
    "select * \n",
    "from airports_staging\n",
    "where iata_code in (select * from non_duplicate_iata_codes)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "clean_airports_df.count()\n",
    "# Returns 8975. Sufficient amount of records for a dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Data model defined in model.png\n",
    "\n",
    "The facts table is created by the immigration data that reside inside the sas_data folder.<br>\n",
    "The countries_dim, modes_dim and states_dim dimension folders are created from I94_SAS_Labels_Descriptions.SAS file after it is parsed into csv files in the label_data_sources folder.<br>\n",
    "The airports_dim dimesion is created from the airport_codes.csv file.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Create csv files for countries, states and modes out from I94_SAS_Labels_Descriptions.SAS file and save them label_data_sources folder.\n",
    "\n",
    "2. Read the immigration data from the sas_data folder and the airport_codes.csv to spark dataframes.\n",
    "\n",
    "3. Read the countries, modes and states data from the label_data_sources folder into spark dataframes.\n",
    "\n",
    "4. Clean the immigration dataframe and the airport dataframe and save the cleaned results into new dataframes (i.e. clean dataframes).\n",
    "\n",
    "5. Create the dimension tables: Write the countries, modes and states dataframes into parquet files. Write the clean airports dataframe into a parquet files. (Lets call these dataframes dimension dataframes)\n",
    "\n",
    "6. Create the fact table: Left join the clean immigration dataframe with all the dimension dataframes and write the result in a paquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "At this point we need to **RESTART THE KERNEL** of the notebook so that we can run the pipeline from the very begining.\n",
    "\n",
    "**! ! ! RESTART THE KERNEL ! ! !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import all the function for the etl\n",
    "from helpful_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Instatiate Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create csv files out of the I94_SAS_Labels_Descriptions.SAS file and save them in label_data_sources folder\n",
    "create_data_sources_from_label_file('I94_SAS_Labels_Descriptions.SAS', 'label_data_sources', 'i94addrl', 'i94cntyl', 'i94model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read immigration data to a spark dataframe\n",
    "immigration_data = './sas_data'\n",
    "\n",
    "immigration_df = spark.read.load(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read airport data to a spark dataframe\n",
    "airport_data = 'airport-codes_csv.csv'\n",
    "\n",
    "airport_df = spark.read.csv(airport_data, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the state data to a spark dataframe\n",
    "state_data = 'label_data_sources/addr.csv'\n",
    "\n",
    "state_df = read_state_data(spark, state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the country data to a spark dataframe\n",
    "country_data = 'label_data_sources/cnty.csv'\n",
    "\n",
    "country_df = read_country_data(spark, country_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the entry mode data to a spark dataframe\n",
    "mode_data = 'label_data_sources/mode.csv'\n",
    "\n",
    "mode_df = read_mode_data(spark, mode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean immigration_df\n",
    "clean_immigration_df = clean_immigration(immigration_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean airport_df\n",
    "clean_airports_df = clean_airports(spark, airport_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimensions\n",
    "\n",
    "# create countries_dim\n",
    "country_df.write.option(\"header\",True).mode(\"overwrite\").parquet(\"star-schema/countries_dim\")\n",
    "\n",
    "# create modes_dim\n",
    "state_df.write.option(\"header\",True).mode(\"overwrite\").parquet(\"star-schema/states_dim\")\n",
    "\n",
    "# create states_dim\n",
    "mode_df.write.option(\"header\",True).mode(\"overwrite\").parquet(\"star-schema/modes_dim\")\n",
    "\n",
    "# create airports_dim\n",
    "final_airports_df = create_airports_dim(spark, clean_airports_df)\n",
    "final_airports_df.write.option(\"header\",True).mode(\"overwrite\").parquet(\"star-schema/airports_dim\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "immigration_fact_df = create_immigration_fact(spark, clean_immigration_df)\n",
    "immigration_fact_df.write.option(\"header\",True).mode(\"overwrite\").parquet(\"star-schema/immigrations_fact\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " \n",
    " 1. Check the counts of our fact and dimension tables and compare them with the counts of the spark dataframes that were created from reading the data initially.\n",
    " 2. Check for the uniqueness of our primary keys in the fact and dimensions tables.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files into spark dataframes\n",
    "\n",
    "immigration_fact = spark.read.parquet(\"star-schema/immigrations_fact\")\n",
    "immigration_fact.createOrReplaceTempView(\"immigration_fact\")\n",
    "\n",
    "states_dim = spark.read.parquet(\"star-schema/states_dim\")\n",
    "states_dim.createOrReplaceTempView(\"states_dim\")\n",
    "\n",
    "modes_dim = spark.read.parquet(\"star-schema/modes_dim\")\n",
    "modes_dim.createOrReplaceTempView(\"modes_dim\")\n",
    "\n",
    "countries_dim = spark.read.parquet(\"star-schema/countries_dim\")\n",
    "countries_dim.createOrReplaceTempView(\"countries_dim\")\n",
    "\n",
    "airports_dim = spark.read.parquet(\"star-schema/airports_dim\")\n",
    "airports_dim.createOrReplaceTempView(\"airports_dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below we check the counts of our fact and dimension tables and compare them with the counts of the spark dataframes that were created from reading the data initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM immigration_fact\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 3096313 which is like the initial dataset count (see check 7 in Explore and Assess the Data Section)\n",
    "# Therefore our we kept all the information from the initial dataset as we wanted\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     289|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM countries_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 289 which is like the initial dataset count (see check 7 in Explore and Assess the Data Section)\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM modes_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 4 which is like the initial dataset count (We know that initial dataser just  had 4 records)\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      55|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM states_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 55 which is like the initial dataset count (see check 7 in Explore and Assess the Data Section)\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    8975|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM airports_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 8975 which is like the cleaned airports dataframe(see Cleaning Steps Section)\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below we check for the uniqueness of our primary keys in the fact and dimensions tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT cicid)\n",
    "FROM immigration_fact\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 3096313 which is the same as the number of records in the table therefore it is unique\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT code)|\n",
      "+--------------------+\n",
      "|                 289|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT code)\n",
    "FROM countries_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 289 which is the same as the number of records in the table therefore it is unique\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT code)|\n",
      "+--------------------+\n",
      "|                   4|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT code)\n",
    "FROM modes_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 4 which is the same as the number of records in the table therefore it is unique\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT code)|\n",
      "+--------------------+\n",
      "|                  55|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT code)\n",
    "FROM states_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 55 which is the same as the number of records in the table therefore it is unique\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT iata_code)|\n",
      "+-------------------------+\n",
      "|                     8975|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT iata_code)\n",
    "FROM airports_dim\n",
    "\"\"\"\n",
    ").show()\n",
    "# Returns 8975 which is the same as the number of records in the table therefore it is unique\n",
    "# Quality check: PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**immigration_fact** <br>\n",
    " |-- cicid: unique id <br>\n",
    " |-- year: year <br>\n",
    " |-- month: month in number format <br>\n",
    " |-- country_code: country code<br>\n",
    " |-- airport_code:  airport code <br>\n",
    " |-- arrival_date: date of arrival in the country<br>\n",
    " |-- mode_code: code for mode of entry<br>\n",
    " |-- state: state code<br>\n",
    " |-- count: value 1 for all records (used for statistics)<br>\n",
    " |-- birth_year: year of birth <br>\n",
    " |-- gender: gender<br>\n",
    " |-- airline: code for airline<br>\n",
    " |-- visa_type: code for visa type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**states_dim**<br>\n",
    " |-- code: 194sas label state code<br>\n",
    " |-- state: state name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**modes_dim**<br>\n",
    " |-- code: 194sas label travel mode code<br>\n",
    " |-- mode: travel mode of entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**countries_dim**<br>\n",
    " |-- code: 194sas label country code<br>\n",
    " |-- country: name of country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**airports_dim**<br>\n",
    " |-- iata_code: airport code<br>\n",
    " |-- identifier: identifier for the airport<br>\n",
    " |-- type: type of airport<br>\n",
    " |-- name: name of airport<br>\n",
    " |-- elevation_ft: elevation <br>\n",
    " |-- continent: continent code <br>\n",
    " |-- country: country code<br>\n",
    " |-- region: region code<br>\n",
    " |-- municipality: municipality<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "\n",
    "* I choose Spark in order to take advantage of it's parrallellization of tasks during transfromations.\n",
    "* I chose to write my data in parquet files in order to take advantage of their columnar storage for better performance on fetching column data. Also we gain storage efficiency as compressed columnar files take less space.\n",
    "* I used pandas for their eye-friendly displaying of dataframes in the Jupiter Notebooks to make the exploration of data easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Propose how often the data should be updated and why.**<br>\n",
    "The data should be updated every day since the analytics team may have questions such as \"Which day of the last week did the most immigrants arrive?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**If the data was incresed by 100x** then I would use a Spark cluster and add more nodes to the cluster as data increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**If the data populates a dashboard that must be updated on a daily basis by 7am every day** then I would use Apache airflow in order to take advantage of its scheduling feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**If the database needed to be accessed by 100+ people** then I would also write another pipeline step that creates and populates the data model in AWS Redshift since it can handle many connections and users as indicated in [AWS website](https://docs.aws.amazon.com/redshift/latest/mgmt/amazon-redshift-limits.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finishing with the example query that is mentioned in the scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                name| count|\n",
      "+--------------------+------+\n",
      "|Miami Internation...|343941|\n",
      "|Murtala Muhammed ...|310163|\n",
      "|San Fernando Airport|152586|\n",
      "|Orlando Executive...|149195|\n",
      "|   Lakefront Airport|136122|\n",
      "|William P Hobby A...|101481|\n",
      "|Hartsfield Jackso...| 92579|\n",
      "|  Al Massira Airport| 80919|\n",
      "|   Dallas Love Field| 71809|\n",
      "|General Edward La...| 57354|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 points of entry that immigrants enter a country\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT a.name, count(*) as count\n",
    "    FROM immigration_fact im\n",
    "    INNER JOIN airports_dim a on a.iata_code = im.airport_code\n",
    "    GROUP BY a.name\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
